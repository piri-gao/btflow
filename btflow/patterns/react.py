"""
BTflow Patterns: ReAct Agent Implementation.

ReAct (Reasoning + Acting) æ˜¯ä¸€ç§ LLM Agent æ¨¡å¼ï¼Œäº¤æ›¿è¿›è¡Œæ¨ç†å’Œå·¥å…·è°ƒç”¨ã€‚

Tree Structure:
    Root (Sequence)
    â”œâ”€â”€ ReActLLMNode     â†’ è°ƒç”¨ LLMï¼Œè¾“å‡º Thought/Action/Final Answer
    â”œâ”€â”€ ToolExecutor     â†’ æ£€æµ‹å¹¶æ‰§è¡Œ Actionï¼ˆæ— åˆ™è·³è¿‡ï¼‰
    â””â”€â”€ CheckFinalAnswer â†’ æå– final_answerï¼Œè§¦å‘ä¸‹ä¸€è½®
"""
import re
import operator
from typing import Annotated, List, Dict, Optional, Type
from pydantic import BaseModel, Field
from py_trees.common import Status
from py_trees.composites import Sequence

from btflow.core.behaviour import AsyncBehaviour
from btflow.core.state import StateManager
from btflow.core.agent import BTAgent
from btflow.core.logging import logger
from btflow.patterns.tools import Tool


# ============ State Schema ============

class ReActState(BaseModel):
    """ReAct Agent çš„çŠ¶æ€å®šä¹‰"""
    # æ¶ˆæ¯å†å²ï¼Œä½¿ç”¨ Reducer è‡ªåŠ¨è¿½åŠ 
    messages: Annotated[List[str], operator.add] = Field(default_factory=list)
    # æœ€ç»ˆç­”æ¡ˆï¼ˆç”¨äºè§¦å‘ wake signal å’Œç»ˆæ­¢åˆ¤æ–­ï¼‰
    final_answer: Optional[str] = None
    # å½“å‰è½®æ•°
    round: int = 0


# ============ ReAct Nodes ============

class ReActLLMNode(AsyncBehaviour):
    """
    ReAct æ¨ç†èŠ‚ç‚¹ï¼šè°ƒç”¨ LLM è¿›è¡Œæ€è€ƒã€‚
    
    è¾“å‡ºæ ¼å¼ï¼ˆReAct æ ‡å‡†æ ¼å¼ï¼‰ï¼š
        Thought: [æ€è€ƒè¿‡ç¨‹]
        Action: [å·¥å…·å]
        Input: [å·¥å…·å‚æ•°]
    
    æˆ–è€…ï¼š
        Thought: [æ€è€ƒè¿‡ç¨‹]
        Final Answer: [æœ€ç»ˆç­”æ¡ˆ]
    """
    
    def __init__(
        self, 
        name: str = "ReActLLM",
        llm_node: Optional[AsyncBehaviour] = None,
        system_prompt: Optional[str] = None
    ):
        """
        Args:
            name: èŠ‚ç‚¹åç§°
            llm_node: åº•å±‚ LLM èŠ‚ç‚¹ï¼ˆå¦‚ GeminiNodeï¼‰ã€‚å¦‚æœæä¾›ï¼Œå°†ä»£ç†è°ƒç”¨ã€‚
            system_prompt: å¦‚æœä¸æä¾› llm_nodeï¼Œéœ€è¦å­ç±»å®ç° _call_llm
        """
        super().__init__(name)
        self.llm_node = llm_node
        self.system_prompt = system_prompt or self._get_default_prompt()
    
    def _get_default_prompt(self) -> str:
        return """You are a helpful assistant that can use tools to answer questions.

You must follow this EXACT format:

Thought: [your reasoning about what to do next]
Action: [tool name]
Input: [tool input]

OR when you have the final answer:

Thought: [your final reasoning]
Final Answer: [your answer to the user]

Available tools will be provided in the conversation.
Always think step by step."""
    
    async def update_async(self) -> Status:
        """è°ƒç”¨ LLM è¿›è¡Œæ¨ç†"""
        try:
            state = self.state_manager.get()
            
            # æ„å»º prompt
            prompt = self._build_prompt(state.messages)
            
            # è°ƒç”¨ LLM
            response = await self._call_llm(prompt)
            
            if not response:
                logger.warning("âš ï¸ [{}] LLM è¿”å›ç©ºå“åº”", self.name)
                return Status.FAILURE
            
            # å†™å…¥æ¶ˆæ¯å†å²
            self.state_manager.update({"messages": [response]})
            
            logger.debug("ğŸ’­ [{}] LLM å“åº”: {}...", self.name, response[:100])
            return Status.SUCCESS
            
        except Exception as e:
            logger.error("ğŸ”¥ [{}] LLM è°ƒç”¨å¤±è´¥: {}", self.name, e)
            return Status.FAILURE
    
    def _build_prompt(self, messages: List[str]) -> str:
        """æ„å»º LLM prompt"""
        return "\n".join(messages)
    
    async def _call_llm(self, prompt: str) -> str:
        """
        è°ƒç”¨ LLMã€‚å­ç±»å¯ä»¥é‡å†™æ­¤æ–¹æ³•ã€‚
        
        å¦‚æœæä¾›äº† llm_nodeï¼Œå°†ä»£ç†è°ƒç”¨ã€‚
        """
        if self.llm_node:
            # ä»£ç†åˆ°åº•å±‚ LLM èŠ‚ç‚¹
            # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å®ç°å¯èƒ½éœ€è¦æ›´å¤æ‚çš„åè°ƒ
            raise NotImplementedError(
                "ä»£ç† LLM èŠ‚ç‚¹æš‚æœªå®ç°ã€‚è¯·å­ç±»åŒ– ReActLLMNode å¹¶é‡å†™ _call_llm æ–¹æ³•ï¼Œ"
                "æˆ–ä½¿ç”¨ ReActGeminiNodeã€‚"
            )
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° _call_llm æ–¹æ³•")


class ReActGeminiNode(AsyncBehaviour):
    """
    ä¸“é—¨ç”¨äº ReAct çš„ Gemini èŠ‚ç‚¹ã€‚
    
    ç›¸æ¯”æ™®é€š GeminiNode çš„æ”¹è¿›ï¼š
    1. åªåœ¨éœ€è¦æ—¶è°ƒç”¨ LLMï¼ˆæœ€åä¸€æ¡æ¶ˆæ¯æ˜¯ Question æˆ– Observationï¼‰
    2. å†™å…¥åŸå§‹ LLM è¾“å‡ºï¼Œä¸åŠ  "Gemini:" å‰ç¼€
    3. é›†æˆ ReAct ä¸“ç”¨çš„ system prompt
    """
    
    def __init__(
        self,
        name: str = "ReActGemini",
        model: str = "gemini-2.5-flash",
        system_prompt: Optional[str] = None,
        tools_description: str = ""
    ):
        """
        Args:
            name: èŠ‚ç‚¹åç§°
            model: Gemini æ¨¡å‹åç§°
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¦‚ä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤ ReAct promptï¼‰
            tools_description: å¯ç”¨å·¥å…·çš„æè¿°
        """
        super().__init__(name)
        self.model = model
        self.tools_description = tools_description
        self.system_prompt = system_prompt or self._get_default_prompt()
        
        # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
        import os
        from google import genai
        
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            logger.warning("âš ï¸ [{}] GOOGLE_API_KEY not found!", self.name)
        
        self.client = genai.Client(api_key=api_key)
    
    def _get_default_prompt(self) -> str:
        tools_section = f"\nAvailable tools:\n{self.tools_description}" if self.tools_description else ""
        
        return f"""You are a helpful assistant that can use tools to answer questions.

You must follow this EXACT format:

Thought: [your reasoning about what to do next]
Action: [tool name]
Input: [tool input]

OR when you have the final answer:

Thought: [your final reasoning]
Final Answer: [your answer to the user]
{tools_section}

IMPORTANT RULES:
1. Always start with "Thought:" to explain your reasoning
2. Use EXACT tool names as shown above (lowercase)
3. After seeing an Observation, continue with another "Thought:"
4. Only use "Final Answer:" when you have the complete answer

Always think step by step."""
    
    def _should_call_llm(self, messages: List[str]) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨ LLMã€‚
        
        åªæœ‰åœ¨ä»¥ä¸‹æƒ…å†µæ‰éœ€è¦è°ƒç”¨ï¼š
        - æ¶ˆæ¯åˆ—è¡¨ä¸ºç©º
        - æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯ Questionï¼ˆç”¨æˆ·è¾“å…¥ï¼‰
        - æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯ Observationï¼ˆå·¥å…·è¾“å‡ºï¼‰
        
        ä¸éœ€è¦è°ƒç”¨çš„æƒ…å†µï¼š
        - æœ€åä¸€æ¡æ¶ˆæ¯åŒ…å« Thought/Actionï¼ˆLLM åˆšåˆšè¾“å‡ºï¼‰
        - æœ€åä¸€æ¡æ¶ˆæ¯åŒ…å« Final Answer
        """
        if not messages:
            return True
        
        last_msg = messages[-1].strip()
        
        # å¦‚æœæœ€åæ¶ˆæ¯æ˜¯ Observationï¼Œéœ€è¦è°ƒç”¨ LLM ç»§ç»­æ€è€ƒ
        if last_msg.startswith("Observation:"):
            return True
        
        # å¦‚æœæœ€åæ¶ˆæ¯æ˜¯ Questionï¼Œéœ€è¦è°ƒç”¨ LLM å¼€å§‹æ€è€ƒ
        if last_msg.startswith("Question:"):
            return True
        
        # å¦‚æœæœ€åæ¶ˆæ¯åŒ…å« Thought æˆ– Action æˆ– Final Answerï¼Œè¯´æ˜ LLM åˆšè¾“å‡ºè¿‡
        if "Thought:" in last_msg or "Action:" in last_msg or "Final Answer:" in last_msg:
            logger.debug("ğŸ“­ [{}] è·³è¿‡ LLM è°ƒç”¨ï¼ˆå·²æœ‰ LLM è¾“å‡ºï¼‰", self.name)
            return False
        
        # å…¶ä»–æƒ…å†µï¼Œè°ƒç”¨ LLM
        return True
    
    def initialise(self) -> None:
        """
        é‡å†™ initialise() æ–¹æ³•ï¼Œåœ¨åˆ›å»ºä»»åŠ¡å‰åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰§è¡Œã€‚
        """
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰§è¡Œ
        self._skip_execution = False
        
        if self.state_manager is not None:
            state = self.state_manager.get()
            if not self._should_call_llm(state.messages):
                self._skip_execution = True
                logger.debug("ğŸ“­ [{}] è·³è¿‡ LLM åˆå§‹åŒ–ï¼ˆä¸éœ€è¦è°ƒç”¨ï¼‰", self.name)
                return
        
        # éœ€è¦æ‰§è¡Œï¼Œè°ƒç”¨çˆ¶ç±»çš„ initialise()
        super().initialise()
    
    def update(self) -> Status:
        """
        é‡å†™ update() æ–¹æ³•ï¼Œé…åˆ initialise() çš„è·³è¿‡é€»è¾‘ã€‚
        """
        if self._skip_execution:
            return Status.SUCCESS
        
        return super().update()
    
    async def update_async(self) -> Status:
        """è°ƒç”¨ Gemini è¿›è¡Œ ReAct æ¨ç†"""
        import asyncio
        from google.genai import types
        
        try:
            state = self.state_manager.get()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨ LLM
            if not self._should_call_llm(state.messages):
                return Status.SUCCESS
            
            # æ„å»º prompt
            prompt_content = "\n".join(state.messages)
            
            logger.debug("ğŸ¤– [{}] è°ƒç”¨ Gemini ({})...", self.name, self.model)
            
            # è°ƒç”¨ API
            response = await asyncio.wait_for(
                self.client.aio.models.generate_content(
                    model=self.model,
                    contents=prompt_content,
                    config=types.GenerateContentConfig(
                        system_instruction=self.system_prompt,
                        temperature=0.7
                    )
                ),
                timeout=60.0  # ReAct å¯èƒ½éœ€è¦æ›´é•¿çš„æ€è€ƒæ—¶é—´
            )
            
            content = response.text.strip()
            
            if not content:
                logger.warning("âš ï¸ [{}] LLM è¿”å›ç©ºå“åº”", self.name)
                return Status.FAILURE
            
            # å†™å…¥åŸå§‹ LLM è¾“å‡ºï¼ˆä¸åŠ å‰ç¼€ï¼‰
            self.state_manager.update({"messages": [content]})
            
            logger.debug("ğŸ’­ [{}] LLM å“åº”:\n{}", self.name, content[:200])
            return Status.SUCCESS
            
        except asyncio.TimeoutError:
            logger.warning("â° [{}] è¯·æ±‚è¶…æ—¶", self.name)
            return Status.FAILURE
        except Exception as e:
            logger.error("ğŸ”¥ [{}] Gemini è°ƒç”¨å¤±è´¥: {}", self.name, e)
            return Status.FAILURE


class ToolExecutor(AsyncBehaviour):
    """
    å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹ï¼šæ£€æµ‹å¹¶æ‰§è¡Œ Actionã€‚
    
    è§£ææœ€åä¸€æ¡æ¶ˆæ¯ä¸­çš„ Action/Inputï¼Œæ‰§è¡Œå¯¹åº”å·¥å…·ï¼Œ
    å°†ç»“æœä½œä¸º Observation å†™å…¥æ¶ˆæ¯å†å²ã€‚
    
    æ— è®ºæ˜¯å¦æ£€æµ‹åˆ° Actionï¼Œéƒ½è¿”å› SUCCESSï¼ˆä¸é˜»å¡ Sequenceï¼‰ã€‚
    """
    
    # ReAct æ ¼å¼æ­£åˆ™
    ACTION_PATTERN = re.compile(
        r"Action:\s*(.+?)\s*\n\s*Input:\s*(.+)",
        re.IGNORECASE | re.DOTALL
    )
    
    def __init__(self, name: str = "ToolExecutor", tools: Optional[List[Tool]] = None):
        """
        Args:
            name: èŠ‚ç‚¹åç§°
            tools: å¯ç”¨å·¥å…·åˆ—è¡¨
        """
        super().__init__(name)
        self.tools: Dict[str, Tool] = {}
        if tools:
            for tool in tools:
                self.register_tool(tool)
    
    def register_tool(self, tool: Tool):
        """æ³¨å†Œå·¥å…·"""
        self.tools[tool.name.lower()] = tool
        logger.debug("ğŸ”§ [{}] æ³¨å†Œå·¥å…·: {}", self.name, tool.name)
    
    def get_tools_description(self) -> str:
        """è·å–æ‰€æœ‰å·¥å…·çš„æè¿°ï¼ˆç”¨äº LLM promptï¼‰"""
        if not self.tools:
            return "No tools available."
        
        descriptions = []
        for name, tool in self.tools.items():
            descriptions.append(f"- {name}: {tool.description}")
        return "\n".join(descriptions)
    
    def _should_execute_tool(self, messages: List[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰§è¡Œå·¥å…·"""
        if not messages:
            return False
        
        last_msg = messages[-1].strip()
        
        # å¦‚æœæœ€åæ¶ˆæ¯æ˜¯ Observationï¼Œä¸éœ€è¦æ‰§è¡Œ
        if last_msg.startswith("Observation:"):
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ Action
        match = self.ACTION_PATTERN.search(last_msg)
        return match is not None
    
    def initialise(self) -> None:
        """
        é‡å†™ initialise() æ–¹æ³•ï¼Œåœ¨åˆ›å»ºä»»åŠ¡å‰åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰§è¡Œã€‚
        """
        self._skip_execution = False
        
        if self.state_manager is not None:
            state = self.state_manager.get()
            if not self._should_execute_tool(state.messages):
                self._skip_execution = True
                logger.debug("ğŸ“­ [{}] è·³è¿‡å·¥å…·åˆå§‹åŒ–ï¼ˆä¸éœ€è¦æ‰§è¡Œï¼‰", self.name)
                return
        
        super().initialise()
    
    def update(self) -> Status:
        """
        é‡å†™ update() æ–¹æ³•ï¼Œé…åˆ initialise() çš„è·³è¿‡é€»è¾‘ã€‚
        """
        if self._skip_execution:
            return Status.SUCCESS
        
        return super().update()
    
    async def update_async(self) -> Status:
        """æ£€æµ‹å¹¶æ‰§è¡Œ Action"""
        state = self.state_manager.get()
        
        if not state.messages:
            return Status.SUCCESS  # æ— æ¶ˆæ¯ï¼Œè·³è¿‡
        
        last_msg = state.messages[-1]
        
        # å¦‚æœæœ€åä¸€æ¡æ¶ˆæ¯å·²ç»æ˜¯ Observationï¼Œè¯´æ˜å·¥å…·å·²æ‰§è¡Œï¼Œè·³è¿‡
        if last_msg.strip().startswith("Observation:"):
            logger.debug("ğŸ“­ [{}] æœ€åæ¶ˆæ¯æ˜¯ Observationï¼Œè·³è¿‡", self.name)
            return Status.SUCCESS
        
        # å°è¯•è§£æ Action
        match = self.ACTION_PATTERN.search(last_msg)
        
        if not match:
            # æ²¡æœ‰æ£€æµ‹åˆ° Actionï¼ˆå¯èƒ½æ˜¯ Final Answerï¼‰ï¼Œè·³è¿‡
            logger.debug("ğŸ“­ [{}] æœªæ£€æµ‹åˆ° Actionï¼Œè·³è¿‡", self.name)
            return Status.SUCCESS
        
        tool_name = match.group(1).strip().lower()
        tool_input = match.group(2).strip()
        
        logger.info("âš™ï¸ [{}] æ£€æµ‹åˆ° Action: {} Input: {}", self.name, tool_name, tool_input)
        
        # æŸ¥æ‰¾å·¥å…·
        tool = self.tools.get(tool_name)
        
        if tool:
            try:
                result = tool.run(tool_input)
                observation = f"Observation: {result}"
            except Exception as e:
                observation = f"Observation: Error executing {tool_name}: {e}"
                logger.warning("âš ï¸ [{}] å·¥å…·æ‰§è¡Œå¤±è´¥: {}", self.name, e)
        else:
            observation = f"Observation: Tool '{tool_name}' not found. Available tools: {list(self.tools.keys())}"
            logger.warning("âš ï¸ [{}] æœªçŸ¥å·¥å…·: {}", self.name, tool_name)
        
        # å†™å…¥ Observation
        self.state_manager.update({"messages": [observation]})
        
        return Status.SUCCESS


class CheckFinalAnswer(AsyncBehaviour):
    """
    ç»ˆæ­¢æ£€æµ‹èŠ‚ç‚¹ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ Final Answerã€‚
    
    - æœ‰ Final Answer â†’ SUCCESSï¼ˆä»»åŠ¡å®Œæˆï¼‰
    - æ—  Final Answer â†’ æ›´æ–°çŠ¶æ€è§¦å‘ä¸‹ä¸€è½® â†’ RUNNING
    - è¶…è¿‡ max_rounds â†’ FAILUREï¼ˆé˜²æ­¢æ­»å¾ªç¯ï¼‰
    """
    
    FINAL_ANSWER_PATTERN = re.compile(
        r"Final Answer:\s*(.+)",
        re.IGNORECASE | re.DOTALL
    )
    
    def __init__(self, name: str = "CheckFinalAnswer", max_rounds: int = 10):
        """
        Args:
            name: èŠ‚ç‚¹åç§°
            max_rounds: æœ€å¤§æ¨ç†è½®æ•°ï¼ˆé˜²æ­¢æ­»å¾ªç¯ï¼‰
        """
        super().__init__(name)
        self.max_rounds = max_rounds
    
    async def update_async(self) -> Status:
        """æ£€æŸ¥å¹¶æå– Final Answer"""
        state = self.state_manager.get()
        current_round = state.round
        
        logger.debug("ğŸ” [{}] æ£€æŸ¥ä¸­... Round={}, Messages={}", 
                   self.name, current_round, len(state.messages))
        
        # å°è¯•æå– Final Answer
        final_answer = self._extract_final_answer(state.messages)
        
        # æ€»æ˜¯æ›´æ–°çŠ¶æ€ï¼ˆè§¦å‘ wake signalï¼‰
        self.state_manager.update({
            "final_answer": final_answer,
            "round": current_round + 1
        })
        
        if final_answer:
            logger.info("âœ… [{}] æ£€æµ‹åˆ° Final Answer: {}...", 
                       self.name, final_answer[:50] if len(final_answer) > 50 else final_answer)
            return Status.SUCCESS
        
        # æ£€æŸ¥æ˜¯å¦è¶…é™
        if current_round + 1 >= self.max_rounds:
            logger.warning("âš ï¸ [{}] è¾¾åˆ°æœ€å¤§è½®æ•° ({}), å¼ºåˆ¶åœæ­¢", self.name, self.max_rounds)
            return Status.FAILURE
        
        logger.debug("ğŸ”„ [{}] ç»§ç»­ä¸‹ä¸€è½® (Round {}/{})", 
                    self.name, current_round + 1, self.max_rounds)
        return Status.RUNNING
    
    def _extract_final_answer(self, messages: List[str]) -> Optional[str]:
        """ä»æ¶ˆæ¯ä¸­æå– Final Answer"""
        if not messages:
            return None
        
        last_msg = messages[-1]
        match = self.FINAL_ANSWER_PATTERN.search(last_msg)
        
        if match:
            return match.group(1).strip()
        return None


# ============ ReAct Agent Factory ============

class ReActAgent:
    """
    ReAct Agent å·¥å‚ç±»ã€‚
    
    æä¾›ä¾¿æ·æ–¹æ³•åˆ›å»ºå®Œæ•´çš„ ReAct Agentã€‚
    
    Example:
        from btflow.patterns import ReActAgent, Tool
        from btflow.nodes.llm import GeminiNode
        
        # å®šä¹‰å·¥å…·
        class MyTool(Tool):
            name = "search"
            description = "Search the web"
            def run(self, input: str) -> str:
                return "search result"
        
        # åˆ›å»º Agent
        agent = ReActAgent.create(
            llm_class=GeminiNode,
            llm_kwargs={"model": "gemini-2.5-flash"},
            tools=[MyTool()],
            max_rounds=10
        )
        
        # è¿è¡Œ
        result = await agent.run({"messages": ["Question: What is 2+2?"]})
    """
    
    @staticmethod
    def create(
        llm_class: Type[AsyncBehaviour],
        llm_kwargs: Optional[Dict] = None,
        tools: Optional[List[Tool]] = None,
        max_rounds: int = 10,
        state_schema: Type[BaseModel] = ReActState
    ) -> BTAgent:
        """
        åˆ›å»º ReAct Agentã€‚
        
        Args:
            llm_class: LLM èŠ‚ç‚¹ç±»ï¼ˆå¦‚ GeminiNodeï¼‰
            llm_kwargs: ä¼ é€’ç»™ LLM èŠ‚ç‚¹çš„å‚æ•°
            tools: å¯ç”¨å·¥å…·åˆ—è¡¨
            max_rounds: æœ€å¤§æ¨ç†è½®æ•°
            state_schema: çŠ¶æ€ Schemaï¼ˆé»˜è®¤ ReActStateï¼‰
        
        Returns:
            é…ç½®å¥½çš„ BTAgent å®ä¾‹
        """
        llm_kwargs = llm_kwargs or {}
        tools = tools or []
        
        # æ„å»ºå·¥å…·æè¿°
        tool_executor = ToolExecutor(name="ToolExecutor", tools=tools)
        tools_desc = tool_executor.get_tools_description()
        
        # æ„å»ºç³»ç»Ÿ prompt
        react_prompt = f"""You are a helpful assistant that can use tools to answer questions.

You must follow this EXACT format:

Thought: [your reasoning about what to do next]
Action: [tool name]
Input: [tool input]

OR when you have the final answer:

Thought: [your final reasoning]
Final Answer: [your answer to the user]

Available tools:
{tools_desc}

Always think step by step. After receiving an Observation, continue with another Thought."""
        
        # åˆ›å»º LLM èŠ‚ç‚¹
        if "system_prompt" not in llm_kwargs:
            llm_kwargs["system_prompt"] = react_prompt
        
        llm_node = llm_class(name="ReActLLM", **llm_kwargs)
        
        # æ„å»ºè¡Œä¸ºæ ‘
        # ä½¿ç”¨ memory=Falseï¼Œæ¯æ¬¡ tick ä»å¤´å¼€å§‹è¯„ä¼°
        # ToolExecutor ä¼šè‡ªåŠ¨è·³è¿‡å·²å¤„ç†çš„ Actionï¼ˆé€šè¿‡æ£€æŸ¥ Observationï¼‰
        root = Sequence(name="ReAct", memory=False, children=[
            llm_node,
            tool_executor,
            CheckFinalAnswer(name="CheckAnswer", max_rounds=max_rounds)
        ])
        
        # åˆ›å»ºçŠ¶æ€ç®¡ç†å™¨
        state_manager = StateManager(schema=state_schema)
        state_manager.initialize({})
        
        # åˆ›å»º Agent
        return BTAgent(root, state_manager)
    
    @staticmethod
    def get_initial_message(question: str, tools_desc: str = "") -> str:
        """ç”Ÿæˆåˆå§‹æ¶ˆæ¯"""
        msg = f"Question: {question}"
        if tools_desc:
            msg = f"Available tools:\n{tools_desc}\n\n{msg}"
        return msg
    
    @staticmethod
    def create_with_gemini(
        tools: Optional[List[Tool]] = None,
        model: str = "gemini-2.5-flash",
        max_rounds: int = 10,
        state_schema: Type[BaseModel] = ReActState
    ) -> BTAgent:
        """
        ä½¿ç”¨ Gemini åˆ›å»º ReAct Agentï¼ˆæ¨èæ–¹å¼ï¼‰ã€‚
        
        ä½¿ç”¨ä¸“é—¨çš„ ReActGeminiNodeï¼Œæ­£ç¡®å¤„ç† ReAct æ ¼å¼ï¼š
        - åªåœ¨éœ€è¦æ—¶è°ƒç”¨ LLMï¼ˆé¿å…é‡å¤è°ƒç”¨ï¼‰
        - è¾“å‡ºåŸå§‹æ ¼å¼ï¼ˆæ—  "Gemini:" å‰ç¼€ï¼‰
        
        Args:
            tools: å¯ç”¨å·¥å…·åˆ—è¡¨
            model: Gemini æ¨¡å‹åç§°
            max_rounds: æœ€å¤§æ¨ç†è½®æ•°
            state_schema: çŠ¶æ€ Schemaï¼ˆé»˜è®¤ ReActStateï¼‰
        
        Returns:
            é…ç½®å¥½çš„ BTAgent å®ä¾‹
            
        Example:
            agent = ReActAgent.create_with_gemini(
                tools=[CalculatorTool(), SearchTool()],
                model="gemini-2.5-flash",
                max_rounds=10
            )
            result = await agent.run({"messages": ["Question: What is 2+2?"]})
        """
        tools = tools or []
        
        # æ„å»ºå·¥å…·æè¿°
        tool_executor = ToolExecutor(name="ToolExecutor", tools=tools)
        tools_desc = tool_executor.get_tools_description()
        
        # åˆ›å»º ReAct ä¸“ç”¨çš„ Gemini èŠ‚ç‚¹
        llm_node = ReActGeminiNode(
            name="ReActLLM",
            model=model,
            tools_description=tools_desc
        )
        
        # æ„å»ºè¡Œä¸ºæ ‘
        # ä½¿ç”¨ memory=Falseï¼Œæ¯æ¬¡ tick ä»å¤´å¼€å§‹è¯„ä¼°
        # ReActGeminiNode ä¼šè‡ªåŠ¨è·³è¿‡ä¸éœ€è¦çš„ LLM è°ƒç”¨
        # ToolExecutor ä¼šè‡ªåŠ¨è·³è¿‡å·²å¤„ç†çš„ Action
        root = Sequence(name="ReAct", memory=False, children=[
            llm_node,
            tool_executor,
            CheckFinalAnswer(name="CheckAnswer", max_rounds=max_rounds)
        ])
        
        # åˆ›å»ºçŠ¶æ€ç®¡ç†å™¨
        state_manager = StateManager(schema=state_schema)
        state_manager.initialize({})
        
        # åˆ›å»º Agent
        return BTAgent(root, state_manager)

