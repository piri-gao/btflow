"""
LLM ChatBot (è¿ç»­å¯¹è¯æ¨¡å¼)
ä½¿ç”¨ BTAgent æ¥å£è¿›è¡Œå¤šè½®å¯¹è¯
"""
import sys
import os
import asyncio
import operator
from typing import Annotated, List
from pydantic import BaseModel, Field
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# Reduce log noise during streaming output
os.environ.setdefault("BTFLOW_LOG_LEVEL", "WARNING")

# ä¸€è¡Œ import æå®šï¼
from btflow import BTAgent, StateManager, Sequence
from btflow.nodes import LLMNode
from btflow.llm import LLMProvider

# === 1. å®šä¹‰çŠ¶æ€ ===
class AgentState(BaseModel):
    messages: Annotated[List[str], operator.add] = Field(default_factory=list)
    step_count: Annotated[int, operator.add] = Field(default=0)
    streaming_output: str = ""

async def main():
    print("\n" + "="*50)
    print("âœ¨ LLM ChatBot (ä½¿ç”¨ BTAgent)")
    print("   è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º")
    print("="*50)

    # === 2. åˆå§‹åŒ– ===
    state_manager = StateManager(schema=AgentState)
    state_manager.initialize({
        "messages": [],
        "step_count": 0
    })

    # === 3. æ„å»ºæ ‘ (ä¸éœ€è¦ä¼  state_managerï¼ŒRunner ä¼šè‡ªåŠ¨æ³¨å…¥) ===
    root = Sequence(name="LLMFlow", memory=True)
    try:
        provider = LLMProvider.default(preference=["gemini", "openai"], base_url=os.getenv("BASE_URL"))
    except RuntimeError as e:
        print(str(e))
        return
    llm_node = LLMNode(
        name="ChatLLM",
        provider=provider,
        model="gemini-2.5-flash",
        system_prompt="ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„ AI åŠ©æ‰‹ã€‚",
        assistant_prefix="Assistant",
        step_key="step_count",
        stream=True,
        streaming_output_key="streaming_output",
    )
    root.add_children([llm_node])

    # === 4. åˆ›å»º BTAgent (æ— éœ€æ‰‹åŠ¨åˆ›å»º Runnerï¼) ===
    agent = BTAgent(root, state_manager)

    # === 5. è¿›å…¥èŠå¤©å¾ªç¯ ===
    last_stream = ""
    streaming_active = False
    def on_state_change():
        nonlocal last_stream, streaming_active
        if not streaming_active:
            return
        current = state_manager.get().streaming_output
        if current and current != last_stream:
            delta = current[len(last_stream):]
            if delta:
                print(delta, end="", flush=True)
            last_stream = current

    state_manager.subscribe(on_state_change)
    while True:
        try:
            user_input = input("\nğŸ‘¤ User: ").strip()
            
            if user_input.lower() in ["exit", "quit", "q"]:
                print("ğŸ‘‹ Bye!")
                break
            if not user_input:
                continue

            # ä½¿ç”¨ BTAgent.run() - è‡ªåŠ¨å¤„ç†æ ‘çŠ¶æ€é‡ç½®
            # reset_tree=True: ä»æ ¹èŠ‚ç‚¹å¼€å§‹æ–°å†³ç­–
            # reset_data=False: ä¿ç•™ messages å†å²
            # Print assistant prefix for streaming
            last_stream = ""
            streaming_active = True
            print("ğŸ¤– ", end="", flush=True)

            await agent.run(
                input_data={"messages": [f"User: {user_input}"], "streaming_output": ""},
                reset_tree=True,
                reset_data=False,
                max_ticks=10
            )
            streaming_active = False

            # æ‰“å°æœ¬æ¬¡å›å¤
            current_msgs = state_manager.get().messages
            if current_msgs and current_msgs[-1].startswith("Assistant:"):
                if last_stream:
                    print()
                else:
                    print(f"ğŸ¤– {current_msgs[-1]}")
                last_stream = ""

        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç”¨æˆ·å¼ºåˆ¶é€€å‡º")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    asyncio.run(main())
